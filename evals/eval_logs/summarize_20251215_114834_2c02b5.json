{
  "agent_name": "summarize",
  "system_prompt": [
    "You are a helpful assistant that answers user questions only based on arxiv research articles.\n\n        When a user asks a query, you first search the index to find relevant results. \n        If you cannot find anything relevant, then you fetch the relevant articles from arxiv using the get_data_to_index tool.\n        Then you perfrom a search using the search tool again.\n\n        You answer the user's question by summarizing all these search results.\n        You always provide at least 3 relevant and appropriate references to all artciles you use when summarizing search results."
  ],
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "get_data_to_index",
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "give me a summary of the latest research in LoRA?",
          "timestamp": "2025-12-15T11:48:22.123402+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers user questions only based on arxiv research articles.\n\n        When a user asks a query, you first search the index to find relevant results. \n        If you cannot find anything relevant, then you fetch the relevant articles from arxiv using the get_data_to_index tool.\n        Then you perfrom a search using the search tool again.\n\n        You answer the user's question by summarizing all these search results.\n        You always provide at least 3 relevant and appropriate references to all artciles you use when summarizing search results.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"LoRA\",\"paper_name\":\"latest research in LoRA\"}",
          "tool_call_id": "call_HgRtcYuEVhWvkhCbmWbqCbDq",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 226,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 23,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-12-15T11:48:22+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-Cn1LqFzEpfTxrJ4S5H6oCES4JzRU4",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [],
          "tool_call_id": "call_HgRtcYuEVhWvkhCbmWbqCbDq",
          "metadata": null,
          "timestamp": "2025-12-15T11:48:23.068149+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers user questions only based on arxiv research articles.\n\n        When a user asks a query, you first search the index to find relevant results. \n        If you cannot find anything relevant, then you fetch the relevant articles from arxiv using the get_data_to_index tool.\n        Then you perfrom a search using the search tool again.\n\n        You answer the user's question by summarizing all these search results.\n        You always provide at least 3 relevant and appropriate references to all artciles you use when summarizing search results.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "get_data_to_index",
          "args": "{\"query\":\"LoRA\",\"paper_name\":\"latest research in LoRA\"}",
          "tool_call_id": "call_xsFoHyxuOcdMXRyRmKsb8ekr",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 257,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 26,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-12-15T11:48:23+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-Cn1LrIOR8uNidtrpL5oTqgkbOzr7d",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "get_data_to_index",
          "content": null,
          "tool_call_id": "call_xsFoHyxuOcdMXRyRmKsb8ekr",
          "metadata": null,
          "timestamp": "2025-12-15T11:48:33.160332+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers user questions only based on arxiv research articles.\n\n        When a user asks a query, you first search the index to find relevant results. \n        If you cannot find anything relevant, then you fetch the relevant articles from arxiv using the get_data_to_index tool.\n        Then you perfrom a search using the search tool again.\n\n        You answer the user's question by summarizing all these search results.\n        You always provide at least 3 relevant and appropriate references to all artciles you use when summarizing search results.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"LoRA\",\"paper_name\":\"latest research in LoRA\"}",
          "tool_call_id": "call_WUnvFMZxBIyENenOqEJST7Ch",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 294,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 23,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-12-15T11:48:33+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-Cn1M1yzugA596Hk9fyO2PwttRLNT0",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "id": "2411.14961v3",
              "title": "LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement",
              "authors": [
                "Jieming Bian",
                "Lei Wang",
                "Letian Zhang",
                "Jie Xu"
              ],
              "published": "2024-11-22T14:19:01Z",
              "summary": "Foundation models (FMs) achieve strong performance across diverse tasks with task-specific fine-tuning, yet full parameter fine-tuning is often computationally prohibitive for large models. Parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing low-rank matrices for tuning fewer parameters. While LoRA allows for efficient fine-tuning, it requires significant data for adaptation, making Federated Learning (FL) an appealing solution due to its privacy-preserving collaborative framework. However, combining LoRA with FL introduces two key challenges: the \\textbf{Server-Side Aggregation Bias}, where server-side averaging of LoRA matrices diverges from the ideal global update, and the \\textbf{Client-Side Initialization Lag}, emphasizing the need for consistent initialization across rounds. Existing approaches address these challenges individually, limiting their effectiveness. We propose LoRA-FAIR, a novel method that tackles both issues by introducing a correction term on the server, enhancing aggregation efficiency and accuracy. LoRA-FAIR maintains computational and communication efficiency, yielding superior performance over state-of-the-art methods. Experimental results on ViT and MLP-Mixer models across large-scale datasets demonstrate that LoRA-FAIR consistently achieves performance improvements in FL settings.",
              "content": "proposed. One of the most impor-\ntant PEFT approaches is low-rank adaptation (LoRA) [16],\nwhich significantly reduces the number of trainable param-\neters by introducing low-rank matrices into the model.\n\nLoRA introduces a parallel branch of trainable low-rank\nmatrices, A and B, to compute the model update \u2206W,\nwhere the ranks of A and B are significantly smaller than\nthe parameters of the pre-trained model, W.\nIn LoRA\nfine-tuning, only A and B are updated, while W remains\nfrozen. This approach greatly reduces the computational re-\nsources required, allowing for efficient fine-tuning with per-\nformance comparable to that of full parameter fine-tuning.\n\nClient 3Client 2Client 1B1B2B3\u2206W1\u2206W2\u2206W3Server\u2206W2\u2206W3\u2206W1\u2206W(2)RefinementPre-trainedWeightsA1A2A3\ufffdB\u2032\ufffdA\ufffdB\u2032\u0394B\ufffdB\ufffdA(1)ReconstructB1A1A1B1 \n \n \n \n \n \n\fDespite these advantages, LoRA still requires substantial\ndata to adapt effectively to specific downstream tasks. How-\never, data from a single device may not be sufficient for\nthis purpose, and fine-tuning often involves multiple de-\nvices that collectively hold the necessary data. This multi-\ndevice setup can raise privacy concerns, as fine-tuning with\ndata from multiple parties may expose sensitive informa-\ntion. Federated Learning (FL) [26] offers a feasible solution\nto this issue. By enabling collaborative learning without\nrequiring data sharing, FL allows participants to fine-tune\nmodels while addressing privacy concerns effectively.\n\nCompared to studies on LoRA fine-tuning in central-\nized settings, fine-tuning LoRA within a FL environment re-\nmains relatively unexplored and presents unique challenges.\nIn this paper, we investigate traditional FL in conjunction\nwith parameter-efficient fine-tuning methods, specifically\nfocusing on LoRA. We argue that fine-tuning LoRA mod-\nules presents two key challenges. First, which we refer to\nas the Challenge 1: Server-Side Aggregation Bias, arises\nbecause averaging the LoRA components (A and B) in-\ndependently at the server does not capture the ideal global\nupdate, potentially introducing noise into the aggregated\nmodel. Second, Challenge 2: Client-Side Initialization\nLag highlights the importance of properly allocating global\nupdates to each client\u2019s pre-trained model and LoRA mod-\nules at the start of the next local training phase to ensure a\nconsistent initialization and mitigate initialization lag. Ex-\nisting FL methods for fine-tuning fail to consider these two\nkey points simultaneously. While some methods, such as\nFLoRA [42], attempt to address Challenge 1 by altering the\naggregation process, they fail to address Challenge 2, which\nlimits the performance to a level comparable to that of di-\nrectly combining FedAvg and LoRA (i.e., FedIT [46]).\n\nTaking both Challenge 1 and Challenge 2 into consider-\nation simultaneously is essential for maximizing the perfor-\nmance of LoRA fine-tuning in a federated learning setting.\nIn this work, we propose a simple yet effective method,\nLoRA-FAIR (short for LoRA with Federated Aggregation\nand Initialization Refinement), designed to tackle both chal-\nlenges concurrently. Specifically, we propose that, on the\nserver side, the original averaged LoRA modules (e.g., \u00afA\nand \u00afB) be kept fixed while introducing a correction term\n\u2206B to \u00afB. This way, the product of the fine-tuned \u00afB + \u2206B\nand \u00afA will closely approximate the ideal server update.\nTo further enhance stability, we introduce a normalization\nterm to ensure that the fine-tuned LoRA module remains\nclose to its original averaged value, thereby preserving the\naverage information collected from each client. Through\nthis simple yet effective design, LoRA-FAIR provides an\napproach that approximates an ideal solution to both chal-\nlenges by preserving the shared average information in the\ninitial model while striving for accurate aggregation on the\nserver side. Consequently, LoRA-FAIR maximizes the ef-\n\nficacy of LoRA fine-tuning within an FL framework, bal-\nancing performance improvements with computational effi-\nciency. Our key contributions are summarized as follows:\n\u2022 We investigate the problem of fine-tuning with LoRA in\nfederated learning setting. Through an initial set of mo-\ntivation experiments, we identify two key challenges that\ncurrently limit the application of LoRA in FL.\n\n\u2022 In response to these challenges, we introduce a novel\nmethod named LoRA-FAIR. LoRA-FAIR is the first in\nthe federated fine-tuning domain to simultaneously con-\nsider both the two challenges while maintaining compu-\ntational and communication efficiency.\n\n\u2022 We conduct experiments using two pre-trained founda-\ntion models, ViT [11] and MLP-Mixer [37], across vari-\nous large-scale datasets. Our proposed LoRA-FAIR con-\nsistently outperforms state-of-the-art methods.\n\n2. Preliminaries\n\n2.1. PEFT with LoRA\n\nLoRA (Low-Rank Adaptation) is a PEFT (parameter-\nefficient fine-tuning) approach that significantly reduces the\nnumber of trainable parameters in large-scale models by in-\ntroducing low-rank matrices into the mod"
            },
            {
              "id": "2506.20629v1",
              "title": "PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models",
              "authors": [
                "Soufiane Hayou",
                "Nikhil Ghosh",
                "Bin Yu"
              ],
              "published": "2025-06-25T17:25:02Z",
              "summary": "Low-Rank Adaptation (LoRA) is a widely used finetuning method for large models. Its small memory footprint allows practitioners to adapt large models to specific tasks at a fraction of the cost of full finetuning. Different modifications have been proposed to enhance its efficiency by, for example, setting the learning rate, the rank, and the initialization. Another improvement axis is adapter placement strategy: when using LoRA, practitioners usually pick module types to adapt with LoRA, such as Query and Key modules. Few works have studied the problem of adapter placement, with nonconclusive results: original LoRA paper suggested placing adapters in attention modules, while other works suggested placing them in the MLP modules. Through an intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a lightweight method that allows automatic identification of module types where LoRA adapters should be placed, given a pretrained model and a finetuning task. We demonstrate that PLoP consistently outperforms, and in the worst case competes, with commonly used placement strategies through comprehensive experiments on supervised finetuning and reinforcement learning for reasoning.",
              "content": "5\n2\n0\n2\n\nn\nu\nJ\n\n5\n2\n\n]\n\nG\nL\n.\ns\nc\n[\n\n1\nv\n9\n2\n6\n0\n2\n.\n6\n0\n5\n2\n:\nv\ni\nX\nr\na\n\nPLoP: Precise LoRA Placement for Efficient\nFinetuning of Large Models\n\nSoufiane Hayou\u2217\nSimons Institute\nUC Berkeley\n\nNikhil Ghosh\nFlatiron Institute\n\nBin Yu\nDept of Statistics\nUC Berkeley\n\nAbstract\n\nLow-Rank Adaptation (LoRA) is a widely used finetuning method for\nlarge models. Its small memory footprint allows practitioners to adapt\nlarge models to specific tasks at a fraction of the cost of full finetuning.\nDifferent modifications have been proposed to enhance its efficiency by,\nfor example, setting the learning rate, the rank, and the initialization. An-\nother improvement axis is adapter placement strategy: when using LoRA,\npractitioners usually pick module types to adapt with LoRA, such as Query\nand Key modules. Few works have studied the problem of adapter place-\nment, with nonconclusive results: original LoRA paper suggested placing\nadapters in attention modules, while other works suggested placing them\nin the MLP modules. Through an intuitive theoretical analysis, we intro-\nduce PLoP (Precise LoRA Placement), a lightweight method that allows\nautomatic identification of module types where LoRA adapters should be\nplaced, given a pretrained model and a finetuning task. We demonstrate\nthat PLoP consistently outperforms, and in the worst case competes, with\ncommonly used placement strategies through comprehensive experiments\non supervised finetuning and reinforcement learning for reasoning.\n\n\u2217\n\nCorresponding author: hayou@berkeley.edu\n\n \n \n \n \n \n \n\f1\n\nIntroduction\n\nLow-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT)\nmethods for large language and vision models. Introduced by [1], LoRA significantly\nreduces the computational and memory requirements of finetuning by freezing the pre-\ntrained model weights and inserting low-rank matrices into the model. This approach has\nenabled the adaptation of production-scale models on limited hardware resources while\nachieving performance comparable to full finetuning.\n\nLoRA improvements. Several works have considered improving LoRA performance by\ne.g. using different learning rates for LoRA modules [2], using normalized updates [3],\nsetting adaptive LoRA rank [4, 5], improving initialization [6], and many other variants,\ne.g. [7, 8, 9, 10, 11, 12].\n\nA critical aspect of LoRA is module selection - deciding which specific components of the\nmodel should receive the low-rank adaptation. In practice, instead of selecting individual\nmodules, one selects module types such as \u201cq_proj\u201d (Query modules), \u201cv_proj\u201d (Value\nmodules), etc. In [1], the authors suggested that inserting LoRA in attention modules\n(Query, Key, and Value) generally yields the best performance among other possible\nplacements. However, in a recent note [13], the same authors further explained the\ndifficulty encountered in LoRA adapter placement, and mentioned that optimal placement\ndepends on pretrained model and the finetuning task. Another work [14] found that for\nsome models, placing LoRA adapters in MLP modules gives better performance. Faced\nwith this confusion, practitioners generally follow one of these guidelines or insert LoRA\nadapters in all modules which comes at a higher finetuning cost. Therefore, it is natural to\nask:\n\nGiven a model and a task, how can we select target module types for LoRA at a reasonable cost?\n\nMemory footprint of LoRA.\nIn practice, LoRA is used to finetune large models with\nrelatively low cost. Consider Llama3.2-3B [15], processing sequences of 2048 tokens with\na batch size of 8. With full finetuning, the memory requirements are substantial. The\nmodel parameters require 12GB in float32, while the Adam optimizer states add another\n24GB. The activations for a single forward pass consume approximately 48GB of memory.\nThis brings the total memory requirement to approximately 84GB necessitating high-end\nGPUs. This becomes more problematic with larger, production-scale models. With LoRA,\nthe computational cost changes dramatically. Using rank-16 adapters on query and value\nmodules introduces only 10 million trainable parameters (0.33% of the model). Notably,\nsince gradients are only computed for the adapter weights, the memory overhead for\ngradient computation is reduced by over 99%. This enables finetuning on a single 24GB\nGPU with the same batch size and sequence length. These low memory footprint is what\nmakes LoRA attractive for finetuning.\n\nAnatomy of a practical module selection method for LoRA finetuning. Based on the\ncomputational constraints outlined above, any practical module selection method for LoRA\nadapter placement must operate within these resource limitations. We identify three main\npillars of a practical method: (i) the method cannot require computing gradients with\nrespect to the full model parameters, as this would defeat the primary purpose of using\nLoRA, (ii) the selection process should not necessitate multiple forward passes through\ndifferent model con"
            },
            {
              "id": "2410.20625v2",
              "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
              "authors": [
                "Jui-Nan Yen",
                "Si Si",
                "Zhao Meng",
                "Felix Yu",
                "Sai Surya Duvvuri",
                "Inderjit S. Dhillon",
                "Cho-Jui Hsieh",
                "Sanjiv Kumar"
              ],
              "published": "2024-10-27T22:57:12Z",
              "summary": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the actual updates to the weights depends on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6\\% accuracy gain on Super-Natural Instructions and 3.5\\% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).",
              "content": "5\n2\n0\n2\n\nl\nu\nJ\n\n6\n1\n\n]\n\nG\nL\n.\ns\nc\n[\n\n2\nv\n5\n2\n6\n0\n2\n.\n0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nPublished as a conference paper at ICLR 2025\n\nLORA DONE RITE: ROBUST INVARIANT TRANSFOR-\nMATION EQUILIBRATION FOR LORA OPTIMIZATION\n\nJui-Nan Yen \u2217 1\n\nSi Si 2\n\nZhao Meng 2\n\nFelix Yu 2\n\nSai Surya Duvvuri \u2217 3\n\nInderjit S. Dhillon 2\n\nCho-Jui Hsieh 12\n\nSanjiv Kumar 2\n\nABSTRACT\n\nLow-rank adaption (LoRA) is a widely used parameter-efficient finetuning method\nfor LLMs that reduces memory requirements. However, current LoRA optimiz-\ners lack transformation invariance, which leads to weight updates that depend on\nhow the two LoRA factors are scaled or rotated. This deficiency leads to ineffi-\ncient learning and sub-optimal solutions in practice. This paper introduces LoRA-\nRITE, a novel adaptive matrix preconditioning method for LoRA optimization,\nwhich achieves transformation invariance while being computationally efficient.\nWe provide theoretical analysis to demonstrate the benefit of our method and con-\nduct experiments on various LLM tasks with different models including Gemma\n2B, 7B, and mT5-XXL. The results demonstrate consistent improvements over ex-\nisting optimizers. For example, replacing Adam with LoRA-RITE during LoRA\nfine-tuning of Gemma-2B yields 4.6% accuracy gain on Super-Natural Instruc-\ntions and 3.5% accuracy gain across four other LLM benchmarks (HellaSwag,\nArcChallenge, GSM8K, OpenBookQA).\n\n1\n\nINTRODUCTION\n\nLow-Rank Adaptation (LoRA) (Hu et al., 2022) is a popular parameter-efficient method for fine-\ntuning Large Language Models (LLMs). By freezing the pretrained weights and injecting trainable\nlow-rank matrices into each layer, LoRA significantly reduces memory requirements and mitigates\noverfitting in limited data settings. More formally, letting W \u2208 Rm\u00d7n be a weight matrix in an\nLLM, LoRA freezes W and introduces a low-rank matrix Z added to W , where Z is represented\nby the multiplication of two rank-r matrices A and B, i.e.,\n\nZ = AB\u22a4 \u2208 Rm\u00d7n, A \u2208 Rm\u00d7r, B \u2208 Rn\u00d7r, r \u226a min(m, n).\n\n(1)\n\nThe matrices A and B will be referred to as LoRA factors in this paper. Recent research has\nexplored numerous variations and improvements over the classic LoRA algorithm (Valipour et al.,\n2023; Zhang et al., 2023b; Liu et al., 2024; Yaras et al., 2024).\n\n1 = A2B\u22a4\n\nDespite being widely used in practice, we find that applying standard optimizers to LoRA leads to\nupdates that are not \u201ctransformation invariant\u201d. By definition of LoRA in (1), the same update Z can\nbe decomposed in multiple ways, i.e., Z = A1B\u22a4\n2 . Ideally, an optimizer should yield the\nsame update to Z regardless of the specific factorization. However, commonly used optimizers with\ndiagonal preconditioners like Adam (Kingma & Ba, 2014), Adagrad (Duchi et al., 2011), RMSProp\n(Tieleman & Hinton, 2012), and even second-order methods like Shampoo (Gupta et al., 2018)\nand CASPR (Duvvuri et al., 2024), violate this principle when applied to LoRA. This violation\nnot only presents a mathematical inconsistency but also leads to significant inefficiencies during\ntraining. In practice, we observe that one LoRA factor often dominates the optimization process,\nreceiving substantial updates while the other remains nearly fixed. Although this can be partially\nmitigated by some recently proposed approaches such as employing different learning rates for the\ntwo factors (Hayou et al., 2024), we ask the question: is there a more principled way to design an\noptimizer that inherently enforces transformation invariance for LoRA?\n\nTo address this challenge, we first prove that any form of diagonal preconditioner cannot achieve\ntransformation invariance, which motivates the use of matrix preconditioners. However, existing\n\n\u2217Work done while at Google. 1UCLA 2Google 3UT Austin\n\n1\n\n \n \n \n \n \n \n\fPublished as a conference paper at ICLR 2025\n\nmatrix preconditioners like Shampoo and CASPR lack transformation invariance and introduce sig-\nnificant computational and memory overhead. To overcome these limitations, we propose LoRA-\nRITE (Robust Invariant Transformation Equilibration), a novel optimizer designed specifically for\nLoRA optimization. LoRA-RITE employs a transformation-invariant preconditioner on the low-\nrank side, achieving transformation invariance without incurring substantial overhead. Furthermore,\nwe demonstrate how to maintain this property when incorporating first and second moments, cru-\ncial for the practical effectiveness of adaptive optimization methods. Empirical evaluations across\nvarious datasets and models confirm the effectiveness of the proposed algorithm.\n\nThe contribution of this paper can be summarized below:\n\n\u2022 We propose LoRA-RITE, the first adaptive matrix preconditioning optimizer for LoRA that\nis transformation-invariant, the property that is lacking in most existing optimizers when\napplied to LoRA. Theoretically, we provide a convergence analysis for our method.\n\n\u2022 Despite utilizing matrix preconditioners, LoRA-RITE achieves little overhead in both\nmemory and time compared to"
            },
            {
              "id": "2411.14961v3",
              "title": "LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement",
              "authors": [
                "Jieming Bian",
                "Lei Wang",
                "Letian Zhang",
                "Jie Xu"
              ],
              "published": "2024-11-22T14:19:01Z",
              "summary": "Foundation models (FMs) achieve strong performance across diverse tasks with task-specific fine-tuning, yet full parameter fine-tuning is often computationally prohibitive for large models. Parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing low-rank matrices for tuning fewer parameters. While LoRA allows for efficient fine-tuning, it requires significant data for adaptation, making Federated Learning (FL) an appealing solution due to its privacy-preserving collaborative framework. However, combining LoRA with FL introduces two key challenges: the \\textbf{Server-Side Aggregation Bias}, where server-side averaging of LoRA matrices diverges from the ideal global update, and the \\textbf{Client-Side Initialization Lag}, emphasizing the need for consistent initialization across rounds. Existing approaches address these challenges individually, limiting their effectiveness. We propose LoRA-FAIR, a novel method that tackles both issues by introducing a correction term on the server, enhancing aggregation efficiency and accuracy. LoRA-FAIR maintains computational and communication efficiency, yielding superior performance over state-of-the-art methods. Experimental results on ViT and MLP-Mixer models across large-scale datasets demonstrate that LoRA-FAIR consistently achieves performance improvements in FL settings.",
              "content": " These models excel across diverse applications, in-\ncluding image generation from prompts, language transla-\ntion, mathematical problem-solving, and natural language\n\n*The first two authors contributed equally to this work, and their names\n\nare listed in random order.\n\nFigure 1. Illustration of LoRA-FAIR. Instead of directly averag-\ning the local LoRA modules Ak and Bk collected from each client\nk on the server side and sending the averaged LoRA modules \u00afA\nand \u00afB back to clients, LoRA-FAIR reconstructs the ideal global\nupdate \u2206W using Eq. (6), finds the residual LoRA module \u2206B\nusing Eq. (8), and replaces \u00afB with the corrected LoRA modules\n\u00afB\u2032 = \u00afB + \u2206B. See details in Sec. 4.\nconversation, among others [49]. However, the standard\nmethod of fine-tuning all model parameters, known as full\nparameter fine-tuning, entails prohibitively high computa-\ntional costs, particularly for large-scale models. To alle-\nviate this problem, parameter-efficient fine-tuning (PEFT)\nmethods [13] have been proposed. One of the most impor-\ntant PEFT approaches is low-rank adaptation (LoRA) [16],\nwhich significantly reduces the number of trainable param-\neters by introducing low-rank matrices into the model.\n\nLoRA introduces a parallel branch of trainable low-rank\nmatrices, A and B, to compute the model update \u2206W,\nwhere the ranks of A and B are significantly smaller than\nthe parameters of the pre-trained model, W.\nIn LoRA\nfine-tuning, only A and B are updated, while W remains\nfrozen. This approach greatly reduces the computational re-\nsources required, allowing for efficient fine-tuning with per-\nformance comparable to that of full parameter fine-tuning.\n\nClient 3Client 2Client 1B1B2B3\u2206W1\u2206W2\u2206W3Server\u2206W2\u2206W3\u2206W1\u2206W(2)RefinementPre-trainedWeightsA1A2A3\ufffdB\u2032\ufffdA\ufffdB\u2032\u0394B\ufffdB\ufffdA(1)ReconstructB1A1A1B1 \n \n \n \n \n \n\fDespite these advantages, LoRA still requires substantial\ndata to adapt effectively to specific downstream tasks. How-\never, data from a single device may not be sufficient for\nthis purpose, and fine-tuning often involves multiple de-\nvices that collectively hold the necessary data. This multi-\ndevice setup can raise privacy concerns, as fine-tuning with\ndata from multiple parties may expose sensitive informa-\ntion. Federated Learning (FL) [26] offers a feasible solution\nto this issue. By enabling collaborative learning without\nrequiring data sharing, FL allows participants to fine-tune\nmodels while addressing privacy concerns effectively.\n\nCompared to studies on LoRA fine-tuning in central-\nized settings, fine-tuning LoRA within a FL environment re-\nmains relatively unexplored and presents unique challenges.\nIn this paper, we investigate traditional FL in conjunction\nwith parameter-efficient fine-tuning methods, specifically\nfocusing on LoRA. We argue that fine-tuning LoRA mod-\nules presents two key challenges. First, which we refer to\nas the Challenge 1: Server-Side Aggregation Bias, arises\nbecause averaging the LoRA components (A and B) in-\ndependently at the server does not capture the ideal global\nupdate, potentially introducing noise into the aggregated\nmodel. Second, Challenge 2: Client-Side Initialization\nLag highlights the importance of properly allocating global\nupdates to each client\u2019s pre-trained model and LoRA mod-\nules at the start of the next local training phase to ensure a\nconsistent initialization and mitigate initialization lag. Ex-\nisting FL methods for fine-tuning fail to consider these two\nkey points simultaneously. While some methods, such as\nFLoRA [42], attempt to address Challenge 1 by altering the\naggregation process, they fail to address Challenge 2, which\nlimits the performance to a level comparable to that of di-\nrectly combining FedAvg and LoRA (i.e., FedIT [46]).\n\nTaking both Challenge 1 and Challenge 2 into consider-\nation simultaneously is essential for maximizing the perfor-\nmance of LoRA fine-tuning in a federated learning setting.\nIn this work, we propose a simple yet effective method,\nLoRA-FAIR (short for LoRA with Federated Aggregation\nand Initialization Refinement), designed to tackle both chal-\nlenges concurrently. Specifically, we propose that, on the\nserver side, the original averaged LoRA modules (e.g., \u00afA\nand \u00afB) be kept fixed while introducing a correction term\n\u2206B to \u00afB. This way, the product of the fine-tuned \u00afB + \u2206B\nand \u00afA will closely approximate the ideal server update.\nTo further enhance stability, we introduce a normalization\nterm to ensure that the fine-tuned LoRA module remains\nclose to its original averaged value, thereby preserving the\naverage information collected from each client. Through\nthis simple yet effective design, LoRA-FAIR provides an\napproach that approximates an ideal solution to both chal-\nlenges by preserving the shared average information in the\ninitial model while striving for accurate aggregation on the\nserver side. Consequently, LoRA-FAIR maximizes the ef-\n\nficacy of LoRA fine-tuning within an FL framework, bal-\nancing performance improvements with computational effi-"
            },
            {
              "id": "2410.20625v2",
              "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
              "authors": [
                "Jui-Nan Yen",
                "Si Si",
                "Zhao Meng",
                "Felix Yu",
                "Sai Surya Duvvuri",
                "Inderjit S. Dhillon",
                "Cho-Jui Hsieh",
                "Sanjiv Kumar"
              ],
              "published": "2024-10-27T22:57:12Z",
              "summary": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the actual updates to the weights depends on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6\\% accuracy gain on Super-Natural Instructions and 3.5\\% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).",
              "content": " on various LLM tasks with different models including Gemma\n2B, 7B, and mT5-XXL. The results demonstrate consistent improvements over ex-\nisting optimizers. For example, replacing Adam with LoRA-RITE during LoRA\nfine-tuning of Gemma-2B yields 4.6% accuracy gain on Super-Natural Instruc-\ntions and 3.5% accuracy gain across four other LLM benchmarks (HellaSwag,\nArcChallenge, GSM8K, OpenBookQA).\n\n1\n\nINTRODUCTION\n\nLow-Rank Adaptation (LoRA) (Hu et al., 2022) is a popular parameter-efficient method for fine-\ntuning Large Language Models (LLMs). By freezing the pretrained weights and injecting trainable\nlow-rank matrices into each layer, LoRA significantly reduces memory requirements and mitigates\noverfitting in limited data settings. More formally, letting W \u2208 Rm\u00d7n be a weight matrix in an\nLLM, LoRA freezes W and introduces a low-rank matrix Z added to W , where Z is represented\nby the multiplication of two rank-r matrices A and B, i.e.,\n\nZ = AB\u22a4 \u2208 Rm\u00d7n, A \u2208 Rm\u00d7r, B \u2208 Rn\u00d7r, r \u226a min(m, n).\n\n(1)\n\nThe matrices A and B will be referred to as LoRA factors in this paper. Recent research has\nexplored numerous variations and improvements over the classic LoRA algorithm (Valipour et al.,\n2023; Zhang et al., 2023b; Liu et al., 2024; Yaras et al., 2024).\n\n1 = A2B\u22a4\n\nDespite being widely used in practice, we find that applying standard optimizers to LoRA leads to\nupdates that are not \u201ctransformation invariant\u201d. By definition of LoRA in (1), the same update Z can\nbe decomposed in multiple ways, i.e., Z = A1B\u22a4\n2 . Ideally, an optimizer should yield the\nsame update to Z regardless of the specific factorization. However, commonly used optimizers with\ndiagonal preconditioners like Adam (Kingma & Ba, 2014), Adagrad (Duchi et al., 2011), RMSProp\n(Tieleman & Hinton, 2012), and even second-order methods like Shampoo (Gupta et al., 2018)\nand CASPR (Duvvuri et al., 2024), violate this principle when applied to LoRA. This violation\nnot only presents a mathematical inconsistency but also leads to significant inefficiencies during\ntraining. In practice, we observe that one LoRA factor often dominates the optimization process,\nreceiving substantial updates while the other remains nearly fixed. Although this can be partially\nmitigated by some recently proposed approaches such as employing different learning rates for the\ntwo factors (Hayou et al., 2024), we ask the question: is there a more principled way to design an\noptimizer that inherently enforces transformation invariance for LoRA?\n\nTo address this challenge, we first prove that any form of diagonal preconditioner cannot achieve\ntransformation invariance, which motivates the use of matrix preconditioners. However, existing\n\n\u2217Work done while at Google. 1UCLA 2Google 3UT Austin\n\n1\n\n \n \n \n \n \n \n\fPublished as a conference paper at ICLR 2025\n\nmatrix preconditioners like Shampoo and CASPR lack transformation invariance and introduce sig-\nnificant computational and memory overhead. To overcome these limitations, we propose LoRA-\nRITE (Robust Invariant Transformation Equilibration), a novel optimizer designed specifically for\nLoRA optimization. LoRA-RITE employs a transformation-invariant preconditioner on the low-\nrank side, achieving transformation invariance without incurring substantial overhead. Furthermore,\nwe demonstrate how to maintain this property when incorporating first and second moments, cru-\ncial for the practical effectiveness of adaptive optimization methods. Empirical evaluations across\nvarious datasets and models confirm the effectiveness of the proposed algorithm.\n\nThe contribution of this paper can be summarized below:\n\n\u2022 We propose LoRA-RITE, the first adaptive matrix preconditioning optimizer for LoRA that\nis transformation-invariant, the property that is lacking in most existing optimizers when\napplied to LoRA. Theoretically, we provide a convergence analysis for our method.\n\n\u2022 Despite utilizing matrix preconditioners, LoRA-RITE achieves little overhead in both\nmemory and time compared to first-order optimizers, especially when the LoRA rank (r)\nis significantly smaller than the original matrix dimensions m and n.\n\n\u2022 The proposed optimizer leads to significantly improved performance across multiple\ndatasets and architectures. For instance, when applied to the GSM8K (Cobbe et al., 2021)\ndataset with a Gemma 7B IT model (Gemma Team et al., 2024), LoRA-RITE achieves a\n55.50% accuracy rate. This surpasses the widely-used Adam optimizer (Kingma & Ba,\n2014) by a substantial margin (48.37%) and even outperforms the second-best optimizer\non this dataset, Lamb (You et al., 2020) (50.64%), by approximately 5%.\n\n2 TRANSFORMATION INVARIANCE FOR LORA OPTIMIZATION\n\nWe now introduce the concept of transformation invariance in LoRA training and demonstrate that\nmost existing optimizers, when applied to LoRA, do not satisfy this property. This deficiency leads\nto inefficient learning in practice.\n\n2.1 DEFINITION OF TRANSFORMATION INVARIANCE\n\nAs introduced in (1), LoRA adds a low-rank m"
            }
          ],
          "tool_call_id": "call_WUnvFMZxBIyENenOqEJST7Ch",
          "metadata": null,
          "timestamp": "2025-12-15T11:48:34.340723+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers user questions only based on arxiv research articles.\n\n        When a user asks a query, you first search the index to find relevant results. \n        If you cannot find anything relevant, then you fetch the relevant articles from arxiv using the get_data_to_index tool.\n        Then you perfrom a search using the search tool again.\n\n        You answer the user's question by summarizing all these search results.\n        You always provide at least 3 relevant and appropriate references to all artciles you use when summarizing search results.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "args": "{\"title\":\"Latest Research in LoRA\",\"summary\":\"Recent research in Low-Rank Adaptation (LoRA) has produced significant advancements focused on enhancing the efficiency and effectiveness of fine-tuning large models. Key contributions include: \\n\\n1. **LoRA-FAIR** proposes a federated learning approach that tackles challenges like Server-Side Aggregation Bias and Client-Side Initialization Lag, enhancing fine-tuning performance in collaborative settings while maintaining efficiency. This method utilizes a correction term during aggregation to improve accuracy.\\n\\n2. **PLoP (Precise LoRA Placement)** provides a method for determining optimal module placement for LoRA adapters, enhancing fine-tuning efficiency across various tasks by automating adapter placement based on pre-trained models and tasks while consistently outperforming traditional methods in performance.\\n\\n3. **LoRA-RITE** introduces a novel optimization method that achieves transformation invariance, addressing inefficiencies in current LoRA optimizers. This approach maintains stability and performance during training by ensuring that updates remain effective regardless of how the low-rank factors are scaled or rotated, resulting in improved accuracy on multiple benchmarks. \\n\\nThe combination of these developments signals a significant evolution in the applicability and performance of LoRA in practical scenarios, especially in federated and resource-constrained environments.\",\"references\":[{\"title\":\"LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement\",\"url\":\"https://arxiv.org/abs/2411.14961\"},{\"title\":\"PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models\",\"url\":\"https://arxiv.org/abs/2506.20629\"},{\"title\":\"LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization\",\"url\":\"https://arxiv.org/abs/2410.20625\"}]}",
          "tool_call_id": "call_zrkw2ybL8R5uDQFa2qg7mMwz",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 8111,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 385,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-12-15T11:48:34+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-Cn1M2Pm4q5xYMYklSM8gJn2MxCqZF",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "content": "Final result processed.",
          "tool_call_id": "call_zrkw2ybL8R5uDQFa2qg7mMwz",
          "metadata": null,
          "timestamp": "2025-12-15T11:48:42.327802+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": null,
      "kind": "request"
    }
  ],
  "usage": {
    "input_tokens": 8888,
    "cache_write_tokens": 0,
    "cache_read_tokens": 0,
    "output_tokens": 457,
    "input_audio_tokens": 0,
    "cache_audio_read_tokens": 0,
    "output_audio_tokens": 0,
    "details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "requests": 4,
    "tool_calls": 3
  },
  "output": {
    "title": "Latest Research in LoRA",
    "summary": "Recent research in Low-Rank Adaptation (LoRA) has produced significant advancements focused on enhancing the efficiency and effectiveness of fine-tuning large models. Key contributions include: \n\n1. **LoRA-FAIR** proposes a federated learning approach that tackles challenges like Server-Side Aggregation Bias and Client-Side Initialization Lag, enhancing fine-tuning performance in collaborative settings while maintaining efficiency. This method utilizes a correction term during aggregation to improve accuracy.\n\n2. **PLoP (Precise LoRA Placement)** provides a method for determining optimal module placement for LoRA adapters, enhancing fine-tuning efficiency across various tasks by automating adapter placement based on pre-trained models and tasks while consistently outperforming traditional methods in performance.\n\n3. **LoRA-RITE** introduces a novel optimization method that achieves transformation invariance, addressing inefficiencies in current LoRA optimizers. This approach maintains stability and performance during training by ensuring that updates remain effective regardless of how the low-rank factors are scaled or rotated, resulting in improved accuracy on multiple benchmarks. \n\nThe combination of these developments signals a significant evolution in the applicability and performance of LoRA in practical scenarios, especially in federated and resource-constrained environments.",
    "references": [
      {
        "title": "LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement",
        "url": "https://arxiv.org/abs/2411.14961"
      },
      {
        "title": "PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models",
        "url": "https://arxiv.org/abs/2506.20629"
      },
      {
        "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
        "url": "https://arxiv.org/abs/2410.20625"
      }
    ]
  }
}